{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create SPARK_HOME and PYLIB env var and update PATH env var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"SPARK_HOME\"] = \"/usr/hdp/current/spark2-client\"\n",
    "os.environ[\"PYLIB\"] = os.environ[\"SPARK_HOME\"] + \"/python/lib\"\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] + \"/py4j-0.10.4-src.zip\")\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] + \"/pyspark.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.conf import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "    .appName(\"SparkML\")\\\n",
    "    .master('local[*]')\\\n",
    "    .enableHiveSupport()\\\n",
    "    .config('spark.sql.warehouse.dir', 'hdfs://bigdata:8020/user/2105B44/spark-warehouse')\\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the dependent libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "from pyspark.sql.functions import isnan, when, count, col, countDistinct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sqoop commands\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### importing data using Sqoop\n",
    "sqoop job \\\n",
    "--create importDataall \\\n",
    "-- import-all-tables \\\n",
    "--connect jdbc:mysql://172.16.0.240/insofe_b44_phd_data \\\n",
    "--username insofeadmin \\\n",
    "--P \\\n",
    "--warehouse-dir '/user/2105B44/B44/PHD_DATASET/' \\\n",
    "-m 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 14 items\r\n",
      "drwx------   - 2105B44 2105B44          0 2018-11-17 11:30 .Trash\r\n",
      "drwxr-xr-x   - 2105B44 2105B44          0 2018-11-04 10:11 .hiveJars\r\n",
      "drwx------   - 2105B44 2105B44          0 2018-11-17 10:55 .staging\r\n",
      "drwxr-xr-x   - 2105B44 2105B44          0 2018-11-17 10:54 B44\r\n",
      "drwxr-xr-x   - 2105B44 2105B44          0 2018-09-23 16:56 Batch44\r\n",
      "drwxr-xr-x   - 2105B44 2105B44          0 2018-11-04 16:37 _sqoop\r\n",
      "-rw-r--r--   3 2105B44 2105B44        263 2018-11-10 11:54 bulkload.txt\r\n",
      "-rw-r--r--   3 2105B44 2105B44   27595240 2018-11-11 11:52 cute_dataset_final1.csv\r\n",
      "drwxr-xr-x   - 2105B44 2105B44          0 2018-11-04 17:27 employeesDB\r\n",
      "drwxr-xr-x   - 2105B44 2105B44          0 2018-11-04 16:13 insofe_empdb\r\n",
      "drwxr-xr-x   - 2105B44 2105B44          0 2018-09-30 15:16 mapreduce-input\r\n",
      "drwxr-xr-x   - 2105B44 2105B44          0 2018-11-04 16:25 new_emp\r\n",
      "drwxr-xr-x   - 2105B44 2105B44          0 2018-11-04 12:28 pig_data\r\n",
      "drwxr-xr-x   - 2105B44 2105B44          0 2018-10-07 12:03 spark-warehouse\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -ls "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Viewing DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat: `/user/2105B44/B44': Is a directory\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -cat /user/2105B44/B44"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat: `/user/2105B44/B44/PHD_DATASET/b44_phd_train': Is a directory\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -cat /user/2105B44/B44/PHD_DATASET/b44_phd_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "drwxr-xr-x   - 2105B44 2105B44          0 2018-11-17 10:54 /user/2105B44/B44/PHD_DATASET/b44_phd_test\r\n",
      "drwxr-xr-x   - 2105B44 2105B44          0 2018-11-17 10:55 /user/2105B44/B44/PHD_DATASET/b44_phd_train\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -ls /user/2105B44/B44/PHD_DATASET/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3,Not sure why there are such bad reviews for this location. As far as Starbucks go, it is pretty average (not especially bad). They get points knocked off due to the small size and lack of sitting/lounging space. You can only walk up or drive thru at this location :( But the drive thru is quick enough. I've never had any issues with my order getting mixed up or attitude from the baristas. I mostly visit in evenings or afternoon and it is never busy.  I've also been during morning hours, but not super early... so I can't comment on early morning rush.Seems like this low rating is partially skewed from people who don't like Starbucks coffee in general... understood, but I wish we could just filter out those reviews.  After all, we all know what Starbucks IS and ISN'T by now. Some of us need an accurate review of the location without coffee snobbery polluting the rating :P (see there ^^^ I included the smiley so my snobby coffee brethren can't take offense)\r\n",
      "5,This is Jersey Boys as in Frankie Valli and the 4 Seasons.  If you're looking for gym, tan, laundry and fist pumping boys you're at the wrong show.  Just sayin. haha    I def. recommend JB if you're looking to be entertained by something other than a cirque, magic or boobie show.   Tickets:  We ended up just purchasing our tix from the Paris desk since we were staying at the Paris.  Too lazy to go to discount tix or look for cheaper ones.  We got the cheapest tickets which were around 60 bucks.  FYI:  I heard the couple next to us ask if they had senior citizen discounts and they were hooked up.  Score!Venue:  Any seat is good since it's not a huge venue.  There aren't any funky columns or obstructions anywhere so no chance of having to watch with your neck turned at a 90 deg angle.  We were sitting the 3rd or 4th row from the back and it was fine.  So unless you feel the need to be up close and personal you're fine getting the cheap seats.Show:   I know I'm getting older when I want to hear music my 63 and 65 year old parents listen to!  Hubs and I were like one of the youngest couples there (in our 30s) but age is nothing but a numba here.  I won't go into details since you should see it for yourself but I will be honest, the beginning is kinda boring.  I remember whispering to the hubs it better pick up or I was ready to take a nappy nap.  But after 15-20 minutes it picks up.  JB is all about how Frankie became Frankie Valli and the 4 Seasons via acting, dancing, and great singing.  I was actually surprised by how many of their hits I knew! So sit back, relax, and enjoy! FYI: Like Rodney S. mentioned, prob not the best show to bring kids to since there is some profanity and gun action going on.....Joyzy to the max!\r\n",
      "1,\"I am curious know of how much they have paid for PR.,,. From my experience, along with others, nothing there has been quality. Staff has been extremely under trained.  I have seen the right ups in the paper along with reviews and wonder how such accommodations have been acquired.  This is not a restaurant that will last in the \"\"Suburbia\"\" of Las Vegas...\"\r\n",
      "3,Wynn oh how I want to love you so... with splendid walls, hip decor, mosaic tiles, and high fashion galore. It's Vegas where everything is a fantasy of overpriced glittery distractions. Weekend gettaway with hubby and we were doing it up big time!!!! I will rate everything individually then give an overall rating at the bottom.The Room- 1.75 starsOur king sized bedroom was not ready when we arrived to a packed lobby at the Wynn on a Saturday afternoon back in March. This is not a good thing in my book, but what can you do. They moved us to a room with two double beds. BIG MISTAKE!!The room and bathroom were both well appointed and the view worthy of a million dollars. However, the beds sucked. They were uncomfortable and not big enough. I guess they really don't want you staying in your room at casinos.When we first walked in the connecting doors to our rooms were open. We attempted and were not able to securely shut them. This definitely felt kinda weird and creepy at the same time.  I was forced to call the front desk to have maintenance come up to shut it for us. He said that the renovations had caused this issue all over the place. hmmmm Better get those contractors back out here then.By far the worst thing about our room were the paper thin walls. We could hear entire conversations and loud partying all night long. What made matters go from bad to even worse was the room vibrating from the base and the continuous throbbing from the disco down below. I don't think we slept a wink either night.  Listen we are all for revelers - revelling --it is Vegas Baby!! BUT not in my bed at 3:30-4 in the morning. I want to be able to sleep and when your paying big bucks  that means the hotel should have some sound proofing going on!!! The FAB Pool - 4.5 stars private loungers, gorgeous decor, great DJs, yummy drinks, & incredible vip service. Best thing about this Resort by FAR!!! Best pool I have been to in Vegas bar none! Sorry Mandalay but the lack of sketchy crowds here trumps ya!The food & drinks inside the hotel 2.0 stars - Overpriced and not that great! We found far better food and drinks elsewhere around town. Hey listen, everyone knows while in Vegas, you are going to get bent over, but you should at least be able to enjoy the ride. Slot machines 3.0 -- I won quite a bit of money, but  my winnings were balanced out by my honey losing his shirt, pants, and everything else at black jack. Darn!!! So much for buying a Chanel purse while I was there.Overall experience 2.8 stars I will round up to 3.0 for the sake of this review. Will we be back? Perhaps but I think we will try out the Encore next door.\r\n",
      "2,I took my kid in for wash/deep cond, she has VERY CURLY hair that tangles EZ, Sasha complained about her back hurting & it taking to long for her to comb out. Her 1st experience & probably last\r\n",
      "5,There is not a single thing about my experience at this hotel that I could complain about. I stayed here for two nights and found it to be clean, quiet, and well-appointed in every way that one would expect of a business-class hotel. My room had a great view of the capital building, and the overall ambiance was professional and inviting. I was impressed with how close it was to the UW campus, as well as a host of great restaurants, State Street, and many of the other Madison attractions.I rented a car while I was in the area which I had to park in the garage below the hotel. It was $12 per day, but considering I live in a city where garage spots will run you $40-$60 a day, I thought this was very fair. I also ate in the restaurant one time, and was pleased with the speed of service and the price of the breakfast as compared to other hotel restaurants. The staff was friendly, and the gym was clean and relatively spacious as hotel gyms go. I would not hesitate to stay here again if I have the chance to go back. Great experience!\r\n",
      "2,Not that authentic. Taqueria Guadalajara is much better tasting and has similar menu.\r\n",
      "3,So, the BF and I ate here on Saturday in the afternoon as I had noticed it on a recent shopping trip to the Biltmore. The decor and ambiance is first class and I found our service to be pretty good, but rich...oh my gosh!! I didn't need to eat anything the rest of the day (which is super unusual for me!) as I was so full from our lunch! It felt like everything was loaded with duck fat, truffle oil and butter, which normally I would not complain about, but this was too much! We were starving when we arrived, so right away we ordered a yummy bottle of champagne (interesting wine list!), the duck confit salad and the truffle fries that we had spotted as we were seated at our table. The fries were delicious and the portion was huge! The salad was very tasty, but we were both surprised by the large duck leg and thigh on the plate with a yummy, ashy piece of Humboldt Fog cheese and small bunch of greens. It was good, but my goodness was it rich! The BF was still hungry (how, I have no idea!) and had been eying the polenta and veggie sides, so we ordered those as well. Both were legitimately dripping in butter. Again, I am a girl who loves a rich meal, but this was a bit much! I will say that as we were leaving, Crush (the attached bar) was packed and both the BF and I commented that we would be up for trying the bar soon as it looked like a fun place to hang out and enjoy their happy hour. All in all, be prepared for over the top, decadent options that individually are bombs, but all together is a little too much!\r\n",
      "1,\"Really, really poor service.  Front desk staff was in a great mood, but that's because they were socializing with a group of guests/friends in the lounge area.  I felt like *I* was the one being rude by interrupting their conversation.  The crib that was supposed to be in our room was not.  We desperately needed it.  We called and then waited, and waited, and waited....  Sound-proofing is non-existent.  Got no sleep.When I expressed my concerns to the staff at check-out, their articulate response was \"\"oh.\"\"Never again.\"\r\n",
      "4,This is little more than just a corner shop, but that shouldn't go against it because it's one of the best corner shops I've seen.At most corner shops you expect to pay high prices and you come in and leave disappointed because you've been priced out of a purchase.  The deals here, however, are outstanding and some of the offers can compete with some of the big chains.It might be little more than just a corner shop, but still a very good place to buy your groceries.\r\n",
      "cat: Unable to write to output stream.\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -cat /user/2105B44/B44/PHD_DATASET/b44_phd_train/part-m-00000 | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it doesnt looks as csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "drwxr-xr-x   - 2105B44 2105B44          0 2018-11-17 10:54 /user/2105B44/B44/PHD_DATASET/b44_phd_test\r\n",
      "drwxr-xr-x   - 2105B44 2105B44          0 2018-11-17 10:55 /user/2105B44/B44/PHD_DATASET/b44_phd_train\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -ls /user/2105B44/B44/PHD_DATASET/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "-rw-r--r--   3 2105B44 2105B44          0 2018-11-17 10:55 /user/2105B44/B44/PHD_DATASET/b44_phd_train/_SUCCESS\r\n",
      "-rw-r--r--   3 2105B44 2105B44   71552229 2018-11-17 10:55 /user/2105B44/B44/PHD_DATASET/b44_phd_train/part-m-00000\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -ls /user/2105B44/B44/PHD_DATASET/b44_phd_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "-rw-r--r--   3 2105B44 2105B44          0 2018-11-17 10:54 /user/2105B44/B44/PHD_DATASET/b44_phd_test/_SUCCESS\r\n",
      "-rw-r--r--   3 2105B44 2105B44   36411941 2018-11-17 10:54 /user/2105B44/B44/PHD_DATASET/b44_phd_test/part-m-00000\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -ls /user/2105B44/B44/PHD_DATASET/b44_phd_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the data and creating a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Schema = StructType([\n",
    "   StructField(\"Rating\", IntegerType(), True),\n",
    "   StructField(\"Review\", StringType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF = spark.read.csv(header=False,\n",
    "                         inferSchema=False,\n",
    "                         schema = df_Schema,\n",
    "                         path=\"/user/2105B44/B44/PHD_DATASET/b44_phd_train\",\n",
    "                         ignoreLeadingWhiteSpace = True, \n",
    "                         ignoreTrailingWhiteSpace = True,\n",
    "                         nullValue =True\n",
    "                          )        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|Rating|              Review|\n",
      "+------+--------------------+\n",
      "|     3|Not sure why ther...|\n",
      "|     5|This is Jersey Bo...|\n",
      "+------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDF.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Rating=3, Review=u'Not sure why there are such bad reviews for this location. As far as Starbucks go'),\n",
       " Row(Rating=5, Review=u\"This is Jersey Boys as in Frankie Valli and the 4 Seasons.  If you're looking for gym\")]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDF.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In trainDF  the review text is not complete. its not reading the complete review text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Rating', 'int'), ('Review', 'string')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDF.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(trainDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100097"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## The data is read into new dataframe trainDF2 for complete review text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF2 = spark.read.csv(header=False,\n",
    "                         inferSchema=True,\n",
    "                         path=\"/user/2105B44/B44/PHD_DATASET/b44_phd_train\",\n",
    "                         ignoreLeadingWhiteSpace = False, \n",
    "                         ignoreTrailingWhiteSpace = False, \n",
    "                          nanValue = \"\", \n",
    "                          nullValue = \"\"  \n",
    "                          )        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|_c0|                 _c1|                 _c2|                 _c3|                 _c4|                 _c5|\n",
      "+---+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|  3|Not sure why ther...| it is pretty ave...| but not super ea...| but I wish we co...| we all know what...|\n",
      "|  5|This is Jersey Bo...|                 tan| laundry and fist...| magic or boobie ...| the beginning is...|\n",
      "+---+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDF2.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### review text is split into 5 coloum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_c0', 'string'),\n",
       " ('_c1', 'string'),\n",
       " ('_c2', 'string'),\n",
       " ('_c3', 'string'),\n",
       " ('_c4', 'string'),\n",
       " ('_c5', 'string')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking Datatypes of coloums\n",
    "trainDF2.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100097"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDF2.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check for na and null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+---+---+\n",
      "|_c0|_c1|_c2|_c3|_c4|_c5|\n",
      "+---+---+---+---+---+---+\n",
      "|  0|  0|  0|  0|  0|  0|\n",
      "+---+---+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check nan values\n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "\n",
    "trainDF2.select([count(when(isnan(c), c)).alias(c) for c in trainDF2.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the data doesnt has any nan values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+-----+-----+-----+\n",
      "|_c0|_c1|  _c2|  _c3|  _c4|  _c5|\n",
      "+---+---+-----+-----+-----+-----+\n",
      "|  0| 30|23937|38763|50917|60350|\n",
      "+---+---+-----+-----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check null values\n",
    "trainDF2.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in trainDF2.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the review text has many null value as seen in coloum _c1 to _c5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## if coloum _c1 is null that means complete review is null for that rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDF2.filter(trainDF2._c1.isNull()).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## There are 30 null in coloum _c1. we have to drop null only in coloum _c1 as that corresponds to null reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# droping rows having null values only in coloum _c1\n",
    "trainDF2 = trainDF2.filter(trainDF2._c1.isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+-----+-----+-----+\n",
      "|_c0|_c1|  _c2|  _c3|  _c4|  _c5|\n",
      "+---+---+-----+-----+-----+-----+\n",
      "|  0|  0|23907|38733|50887|60320|\n",
      "+---+---+-----+-----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check null values after droping null from coloum _c1\n",
    "trainDF2.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in trainDF2.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 30 rows are droped "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|_c0|                 _c1|                 _c2|                 _c3|                 _c4|                 _c5|\n",
      "+---+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|  3|Not sure why ther...| it is pretty ave...| but not super ea...| but I wish we co...| we all know what...|\n",
      "|  5|This is Jersey Bo...|                 tan| laundry and fist...| magic or boobie ...| the beginning is...|\n",
      "|  1|\"I am curious kno...|                null|                null|                null|                null|\n",
      "+---+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDF2.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+-----+-----+-----+\n",
      "|_c0|_c1|  _c2|  _c3|  _c4|  _c5|\n",
      "+---+---+-----+-----+-----+-----+\n",
      "|  0|  0|23907|38733|50887|60320|\n",
      "+---+---+-----+-----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check null values\n",
    "trainDF2.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in trainDF2.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fill null values to avoid getting empty reviews after concatinate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill \"_\" to null cell\n",
    "trainDF3 = trainDF2.na.fill(\"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+---+---+\n",
      "|_c0|_c1|_c2|_c3|_c4|_c5|\n",
      "+---+---+---+---+---+---+\n",
      "|  0|  0|  0|  0|  0|  0|\n",
      "+---+---+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check null values\n",
    "trainDF3.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in trainDF3.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100067"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDF3.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A new dataframe is created with filling \"_\"  to empty cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We have to concatnate all 5 coloums to single colum of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_c0=u'3', _c1=u'Not sure why there are such bad reviews for this location. As far as Starbucks go', _c2=u\" it is pretty average (not especially bad). They get points knocked off due to the small size and lack of sitting/lounging space. You can only walk up or drive thru at this location :( But the drive thru is quick enough. I've never had any issues with my order getting mixed up or attitude from the baristas. I mostly visit in evenings or afternoon and it is never busy.  I've also been during morning hours\", _c3=u\" but not super early... so I can't comment on early morning rush.Seems like this low rating is partially skewed from people who don't like Starbucks coffee in general... understood\", _c4=u' but I wish we could just filter out those reviews.  After all', _c5=u\" we all know what Starbucks IS and ISN'T by now. Some of us need an accurate review of the location without coffee snobbery polluting the rating :P (see there ^^^ I included the smiley so my snobby coffee brethren can't take offense)\", reviews=u\"Not sure why there are such bad reviews for this location. As far as Starbucks go_ it is pretty average (not especially bad). They get points knocked off due to the small size and lack of sitting/lounging space. You can only walk up or drive thru at this location :( But the drive thru is quick enough. I've never had any issues with my order getting mixed up or attitude from the baristas. I mostly visit in evenings or afternoon and it is never busy.  I've also been during morning hours\")]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import functions as sf\n",
    "\n",
    "# concatinating _c1 and _c2 to new coloum reviews\n",
    "\n",
    "trainDF3 = trainDF3.withColumn('reviews', \n",
    "                    sf.concat(sf.col('_c1'),sf.lit('_'), sf.col('_c2')))\n",
    "trainDF3.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concanitaing all coloums 5 coloums to reviews coloum\n",
    "\n",
    "trainDF3 = trainDF3.withColumn('reviews', \n",
    "                    sf.concat(sf.col('reviews'),sf.lit('_'), sf.col('_c3')))\n",
    "\n",
    "trainDF3 = trainDF3.withColumn('reviews', \n",
    "                    sf.concat(sf.col('reviews'),sf.lit('_'), sf.col('_c4')))\n",
    "\n",
    "trainDF3 = trainDF3.withColumn('reviews', \n",
    "                    sf.concat(sf.col('reviews'),sf.lit('_'), sf.col('_c5')))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_c0=u'3', _c1=u'Not sure why there are such bad reviews for this location. As far as Starbucks go', _c2=u\" it is pretty average (not especially bad). They get points knocked off due to the small size and lack of sitting/lounging space. You can only walk up or drive thru at this location :( But the drive thru is quick enough. I've never had any issues with my order getting mixed up or attitude from the baristas. I mostly visit in evenings or afternoon and it is never busy.  I've also been during morning hours\", _c3=u\" but not super early... so I can't comment on early morning rush.Seems like this low rating is partially skewed from people who don't like Starbucks coffee in general... understood\", _c4=u' but I wish we could just filter out those reviews.  After all', _c5=u\" we all know what Starbucks IS and ISN'T by now. Some of us need an accurate review of the location without coffee snobbery polluting the rating :P (see there ^^^ I included the smiley so my snobby coffee brethren can't take offense)\", reviews=u\"Not sure why there are such bad reviews for this location. As far as Starbucks go_ it is pretty average (not especially bad). They get points knocked off due to the small size and lack of sitting/lounging space. You can only walk up or drive thru at this location :( But the drive thru is quick enough. I've never had any issues with my order getting mixed up or attitude from the baristas. I mostly visit in evenings or afternoon and it is never busy.  I've also been during morning hours_ but not super early... so I can't comment on early morning rush.Seems like this low rating is partially skewed from people who don't like Starbucks coffee in general... understood_ but I wish we could just filter out those reviews.  After all_ we all know what Starbucks IS and ISN'T by now. Some of us need an accurate review of the location without coffee snobbery polluting the rating :P (see there ^^^ I included the smiley so my snobby coffee brethren can't take offense)\")]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDF3.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|_c0|                 _c1|                 _c2|                 _c3|                 _c4|                 _c5|             reviews|\n",
      "+---+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|  3|Not sure why ther...| it is pretty ave...| but not super ea...| but I wish we co...| we all know what...|Not sure why ther...|\n",
      "|  5|This is Jersey Bo...|                 tan| laundry and fist...| magic or boobie ...| the beginning is...|This is Jersey Bo...|\n",
      "|  1|\"I am curious kno...|                   _|                   _|                   _|                   _|\"I am curious kno...|\n",
      "|  3|Wynn oh how I wan...|           hip decor|        mosaic tiles| and high fashion...| but what can you...|Wynn oh how I wan...|\n",
      "|  2|I took my kid in ...| she has VERY CUR...| Sasha complained...|                   _|                   _|I took my kid in ...|\n",
      "|  5|There is not a si...|               quiet| and well-appoint...| and the overall ...| as well as a hos...|There is not a si...|\n",
      "|  2|Not that authenti...|                   _|                   _|                   _|                   _|Not that authenti...|\n",
      "|  3|                  So| the BF and I ate...| but rich...oh my...| truffle oil and ...| which normally I...|So_ the BF and I ...|\n",
      "|  1|\"Really, really p...|                   _|                   _|                   _|                   _|\"Really, really p...|\n",
      "|  4|This is little mo...| but that shouldn...|             however| are outstanding ...| but still a very...|This is little mo...|\n",
      "+---+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDF3.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop 5 coloums '_c1', '_c2', '_c3', '_c4', '_c5'\n",
    "trainDF3 = trainDF3.drop('_c1', '_c2', '_c3', '_c4', '_c5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_c0=u'3', reviews=u\"Not sure why there are such bad reviews for this location. As far as Starbucks go_ it is pretty average (not especially bad). They get points knocked off due to the small size and lack of sitting/lounging space. You can only walk up or drive thru at this location :( But the drive thru is quick enough. I've never had any issues with my order getting mixed up or attitude from the baristas. I mostly visit in evenings or afternoon and it is never busy.  I've also been during morning hours_ but not super early... so I can't comment on early morning rush.Seems like this low rating is partially skewed from people who don't like Starbucks coffee in general... understood_ but I wish we could just filter out those reviews.  After all_ we all know what Starbucks IS and ISN'T by now. Some of us need an accurate review of the location without coffee snobbery polluting the rating :P (see there ^^^ I included the smiley so my snobby coffee brethren can't take offense)\")]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDF3.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "|_c0|             reviews|\n",
      "+---+--------------------+\n",
      "|  3|Not sure why ther...|\n",
      "|  5|This is Jersey Bo...|\n",
      "|  1|\"I am curious kno...|\n",
      "|  3|Wynn oh how I wan...|\n",
      "|  2|I took my kid in ...|\n",
      "|  5|There is not a si...|\n",
      "|  2|Not that authenti...|\n",
      "|  3|So_ the BF and I ...|\n",
      "|  1|\"Really, really p...|\n",
      "|  4|This is little mo...|\n",
      "+---+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDF3.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_c0', 'string'), ('reviews', 'string')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDF3.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "|_c0|             reviews|\n",
      "+---+--------------------+\n",
      "|  3|Not sure why ther...|\n",
      "|  5|This is Jersey Bo...|\n",
      "+---+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDF3.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing packages\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer,RegexTokenizer, StopWordsRemover, CountVectorizer\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer\n",
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# regular expression tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the review text is tokenized\n",
    "reg_tokenizer = RegexTokenizer(inputCol=\"reviews\", outputCol=\"words\", pattern=\"[^a-zA-Z]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop words are removed\n",
    "stop_words_remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bag of words count is created\n",
    "count_vectors = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\", vocabSize=10000, minDF=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_stringIdx = StringIndexer(inputCol = \"_c0\", outputCol = \"label\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[label_stringIdx,reg_tokenizer, stop_words_remover, count_vectors])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-----+--------------------+--------------------+--------------------+\n",
      "|_c0|             reviews|label|               words|            filtered|            features|\n",
      "+---+--------------------+-----+--------------------+--------------------+--------------------+\n",
      "|  3|Not sure why ther...|  2.0|[not, sure, why, ...|[sure, bad, revie...|(10000,[3,5,9,12,...|\n",
      "|  5|This is Jersey Bo...|  4.0|[this, is, jersey...|[jersey, boys, fr...|(10000,[2,3,4,9,1...|\n",
      "|  1|\"I am curious kno...|  0.0|[i, am, curious, ...|[curious, know, m...|(10000,[25,29,42,...|\n",
      "|  3|Wynn oh how I wan...|  2.0|[wynn, oh, how, i...|[wynn, oh, want, ...|(10000,[2,8,11,12...|\n",
      "|  2|I took my kid in ...|  1.0|[i, took, my, kid...|[took, kid, wash,...|(10000,[11,58,72,...|\n",
      "+---+--------------------+-----+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fit the pipeline to training documents.\n",
    "pipelineFit = pipeline.fit(trainDF3)\n",
    "dataset = pipelineFit.transform(trainDF3)\n",
    "dataset.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_c0=u'3', reviews=u\"Not sure why there are such bad reviews for this location. As far as Starbucks go_ it is pretty average (not especially bad). They get points knocked off due to the small size and lack of sitting/lounging space. You can only walk up or drive thru at this location :( But the drive thru is quick enough. I've never had any issues with my order getting mixed up or attitude from the baristas. I mostly visit in evenings or afternoon and it is never busy.  I've also been during morning hours_ but not super early... so I can't comment on early morning rush.Seems like this low rating is partially skewed from people who don't like Starbucks coffee in general... understood_ but I wish we could just filter out those reviews.  After all_ we all know what Starbucks IS and ISN'T by now. Some of us need an accurate review of the location without coffee snobbery polluting the rating :P (see there ^^^ I included the smiley so my snobby coffee brethren can't take offense)\", label=2.0, words=[u'not', u'sure', u'why', u'there', u'are', u'such', u'bad', u'reviews', u'for', u'this', u'location', u'as', u'far', u'as', u'starbucks', u'go', u'it', u'is', u'pretty', u'average', u'not', u'especially', u'bad', u'they', u'get', u'points', u'knocked', u'off', u'due', u'to', u'the', u'small', u'size', u'and', u'lack', u'of', u'sitting', u'lounging', u'space', u'you', u'can', u'only', u'walk', u'up', u'or', u'drive', u'thru', u'at', u'this', u'location', u'but', u'the', u'drive', u'thru', u'is', u'quick', u'enough', u'i', u've', u'never', u'had', u'any', u'issues', u'with', u'my', u'order', u'getting', u'mixed', u'up', u'or', u'attitude', u'from', u'the', u'baristas', u'i', u'mostly', u'visit', u'in', u'evenings', u'or', u'afternoon', u'and', u'it', u'is', u'never', u'busy', u'i', u've', u'also', u'been', u'during', u'morning', u'hours', u'but', u'not', u'super', u'early', u'so', u'i', u'can', u't', u'comment', u'on', u'early', u'morning', u'rush', u'seems', u'like', u'this', u'low', u'rating', u'is', u'partially', u'skewed', u'from', u'people', u'who', u'don', u't', u'like', u'starbucks', u'coffee', u'in', u'general', u'understood', u'but', u'i', u'wish', u'we', u'could', u'just', u'filter', u'out', u'those', u'reviews', u'after', u'all', u'we', u'all', u'know', u'what', u'starbucks', u'is', u'and', u'isn', u't', u'by', u'now', u'some', u'of', u'us', u'need', u'an', u'accurate', u'review', u'of', u'the', u'location', u'without', u'coffee', u'snobbery', u'polluting', u'the', u'rating', u'p', u'see', u'there', u'i', u'included', u'the', u'smiley', u'so', u'my', u'snobby', u'coffee', u'brethren', u'can', u't', u'take', u'offense'], filtered=[u'sure', u'bad', u'reviews', u'location', u'far', u'starbucks', u'go', u'pretty', u'average', u'especially', u'bad', u'get', u'points', u'knocked', u'due', u'small', u'size', u'lack', u'sitting', u'lounging', u'space', u'walk', u'drive', u'thru', u'location', u'drive', u'thru', u'quick', u'enough', u've', u'never', u'issues', u'order', u'getting', u'mixed', u'attitude', u'baristas', u'mostly', u'visit', u'evenings', u'afternoon', u'never', u'busy', u've', u'also', u'morning', u'hours', u'super', u'early', u'comment', u'early', u'morning', u'rush', u'seems', u'like', u'low', u'rating', u'partially', u'skewed', u'people', u'like', u'starbucks', u'coffee', u'general', u'understood', u'wish', u'filter', u'reviews', u'know', u'starbucks', u'isn', u'us', u'need', u'accurate', u'review', u'location', u'without', u'coffee', u'snobbery', u'polluting', u'rating', u'p', u'see', u'included', u'smiley', u'snobby', u'coffee', u'brethren', u'take', u'offense'], features=SparseVector(10000, {3: 2.0, 5: 1.0, 9: 1.0, 12: 1.0, 14: 2.0, 20: 1.0, 26: 1.0, 27: 1.0, 28: 2.0, 31: 1.0, 44: 1.0, 51: 2.0, 69: 1.0, 73: 1.0, 82: 3.0, 83: 1.0, 91: 1.0, 131: 1.0, 136: 1.0, 157: 1.0, 169: 1.0, 179: 1.0, 183: 3.0, 186: 1.0, 199: 2.0, 211: 1.0, 241: 1.0, 247: 1.0, 254: 1.0, 259: 1.0, 295: 1.0, 310: 1.0, 339: 1.0, 356: 1.0, 358: 1.0, 377: 2.0, 413: 1.0, 414: 1.0, 423: 2.0, 427: 1.0, 448: 2.0, 529: 1.0, 569: 1.0, 612: 1.0, 727: 1.0, 730: 2.0, 731: 1.0, 787: 1.0, 792: 1.0, 806: 1.0, 874: 1.0, 904: 1.0, 992: 2.0, 999: 1.0, 1087: 3.0, 1138: 1.0, 1159: 1.0, 1601: 1.0, 1667: 1.0, 3269: 1.0, 3353: 1.0, 3376: 1.0, 4059: 1.0, 4595: 1.0, 4743: 1.0, 4909: 1.0, 5291: 1.0, 6787: 1.0, 7483: 1.0}))]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_c0', 'string'),\n",
       " ('reviews', 'string'),\n",
       " ('label', 'double'),\n",
       " ('words', 'array<string>'),\n",
       " ('filtered', 'array<string>'),\n",
       " ('features', 'vector')]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# importing test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdf_Schema = StructType([\n",
    "   StructField(\"rating\", StringType(), True),\n",
    "   StructField(\"reviews\", StringType(), True)])\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDF = spark.read.csv(header=False,\n",
    "                         inferSchema=False,\n",
    "                         schema = testdf_Schema,\n",
    "                         path=\"/user/2105B44/B44/PHD_DATASET/b44_phd_test\",\n",
    "                         ignoreLeadingWhiteSpace = True, \n",
    "                         ignoreTrailingWhiteSpace = True,\n",
    "                         nullValue =True\n",
    "                          )        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|rating|             reviews|\n",
      "+------+--------------------+\n",
      "|     1|\"I got 'new' tire...|\n",
      "|     1|Don't waste your ...|\n",
      "|     1|All I can say is ...|\n",
      "+------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testDF.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('rating', 'string'), ('reviews', 'string')]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testDF.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "|rating|reviews|\n",
      "+------+-------+\n",
      "|     0|      0|\n",
      "+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check nan values\n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "\n",
    "testDF.select([count(when(isnan(c), c)).alias(c) for c in testDF.columns]).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "|rating|reviews|\n",
      "+------+-------+\n",
      "|     0|     12|\n",
      "+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# check null values\n",
    "testDF.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in testDF.columns]).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# droping rows having null values only in coloum reviews\n",
    "testDF = testDF.filter(testDF.reviews.isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "|rating|reviews|\n",
      "+------+-------+\n",
      "|     0|      0|\n",
      "+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# check null values\n",
    "testDF.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in testDF.columns]).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+-----+--------------------+--------------------+--------------------+\n",
      "|rating|             reviews|label|               words|            filtered|            features|\n",
      "+------+--------------------+-----+--------------------+--------------------+--------------------+\n",
      "|     1|\"I got 'new' tire...|  2.0|[i, got, new, tir...|[got, new, tires,...|(10000,[7,8,10,11...|\n",
      "|     1|Don't waste your ...|  2.0|[don, t, waste, y...|[waste, time, two...|(10000,[5,7,16,25...|\n",
      "|     1|All I can say is ...|  2.0|[all, i, can, say...|[say, worst, peop...|(10000,[0,25,47,6...|\n",
      "|     1|I have been to th...|  2.0|[i, have, been, t...|[restaurant, twic...|(10000,[0,1,7,10,...|\n",
      "|     1|Food was NOT GOOD...|  2.0|[food, was, not, ...|[food, good, husb...|(10000,[1,2,3,7,8...|\n",
      "+------+--------------------+-----+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# importing packages\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer,RegexTokenizer, StopWordsRemover, CountVectorizer\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# the review text is tokenized\n",
    "reg_tokenizer = RegexTokenizer(inputCol=\"reviews\", outputCol=\"words\", pattern=\"[^a-zA-Z]\")\n",
    "\n",
    "# stop words are removed\n",
    "stop_words_remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
    "\n",
    "# bag of words count is created\n",
    "count_vectors = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\", vocabSize=10000, minDF=5)\n",
    "\n",
    "label_stringIdx = StringIndexer(inputCol = \"rating\", outputCol = \"label\")\n",
    "\n",
    "pipeline_test = Pipeline(stages=[label_stringIdx,reg_tokenizer, stop_words_remover, count_vectors])\n",
    "\n",
    "# Fit the pipeline to test data.\n",
    "pipelineFit_test = pipeline_test.fit(testDF)\n",
    "test_dataset = pipelineFit_test.transform(testDF)\n",
    "test_dataset.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import package\n",
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## logistic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building logestic model\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting model to test data\n",
    "lrModel = lr.fit(test_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting \n",
    "predictions = lrModel.transform(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.778350927703135"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = predictions.filter(predictions.label == predictions.prediction).count() / float(testDF.count())\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('the prediction accuracy is ', 77.8350927703135)\n"
     ]
    }
   ],
   "source": [
    "print(\"the prediction accuracy is \",accuracy*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ('the prediction accuracy is ', 77.8350927703135)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (16, 9)\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_km = test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+-----+--------------------+--------------------+--------------------+\n",
      "|rating|             reviews|label|               words|            filtered|            features|\n",
      "+------+--------------------+-----+--------------------+--------------------+--------------------+\n",
      "|     1|\"I got 'new' tire...|  2.0|[i, got, new, tir...|[got, new, tires,...|(10000,[7,8,10,11...|\n",
      "|     1|Don't waste your ...|  2.0|[don, t, waste, y...|[waste, time, two...|(10000,[5,7,16,25...|\n",
      "+------+--------------------+-----+--------------------+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_km.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
